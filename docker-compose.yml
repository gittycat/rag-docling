services:
  # WebApp - SvelteKit frontend for RAG system
  webapp:
    build:
      context: ./services/webapp
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - "8000:3000"
    environment:
      - ORIGIN=http://localhost:8000
      - RAG_SERVER_URL=http://rag-server:8001
      - MAX_UPLOAD_SIZE=80
    depends_on:
      rag-server:
        condition: service_healthy
    networks:
      - public
      - private

  # RAG Server - API service for document processing and querying
  rag-server:
    build:
      context: .
      dockerfile: ./services/rag_server/Dockerfile
    user: "1000:1000"
    restart: unless-stopped
    ports:
      - "8001:8001"
    environment:
      - CHROMADB_URL=http://chromadb:8000
      # LLM Provider Configuration
      # Supported: ollama, openai, anthropic, google, deepseek, moonshot
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_MODEL=${LLM_MODEL:-gemma3:4b}
      - LLM_API_KEY=${LLM_API_KEY:-}
      - LLM_BASE_URL=${LLM_BASE_URL:-}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-120}
      # Ollama-specific (used when LLM_PROVIDER=ollama)
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-10m}
      # Embedding Model
      - EMBEDDING_MODEL=nomic-embed-text:latest
      - ENABLE_RERANKER=true
      - RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
      - RETRIEVAL_TOP_K=10
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=WARNING
      - MAX_UPLOAD_SIZE=80                # Max file upload size in MB
      # Phase 2: High-Impact Retrieval Improvements
      - ENABLE_HYBRID_SEARCH=true         # BM25 + Vector with RRF fusion
      - RRF_K=60                          # Reciprocal Rank Fusion k parameter (optimal: 60)
      - ENABLE_CONTEXTUAL_RETRIEVAL=false # Anthropic method: 49% reduction in retrieval failures (WARNING: 85% slower preprocessing when enabled)
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      chromadb:
        condition: service_started
      redis:
        condition: service_started
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8001/health', timeout=2)"]
      interval: 2s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - private
      - public
    volumes:
      - ./data/indexed_documents:/app/documents
      - docs_repo:/tmp/shared
      - huggingface_cache:/home/appuser/.cache/huggingface
      - documents_data:/data/documents
    secrets:
      - ollama_config

  # ChromaDB Vector Database (private network only)
  chromadb:
    image: chromadb/chroma:latest
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - CHROMA_LOG_CONFIG=/chroma/log_config.yml
    volumes:
      - chromadb_data:/chroma/chroma
      - ./services/chromadb/log_config.yml:/chroma/log_config.yml:ro
    networks:
      - private
    restart: unless-stopped

  # Redis - Message broker and result backend for Celery
  redis:
    image: redis:8-alpine
    command: redis-server --loglevel warning --save "" --appendonly no
    networks:
      - private
    restart: unless-stopped

  # Celery Worker - Async document processing
  celery-worker:
    build:
      context: .
      dockerfile: ./services/rag_server/Dockerfile
    command: [".venv/bin/celery", "--quiet", "-A", "infrastructure.tasks.celery_app", "worker", "--concurrency=1", "--without-mingle", "--without-gossip"]
    user: "1000:1000"
    restart: unless-stopped
    environment:
      - CHROMADB_URL=http://chromadb:8000
      # LLM Provider Configuration (same as rag-server)
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_MODEL=${LLM_MODEL:-gemma3:4b}
      - LLM_API_KEY=${LLM_API_KEY:-}
      - LLM_BASE_URL=${LLM_BASE_URL:-}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-120}
      # Ollama-specific (used when LLM_PROVIDER=ollama)
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-10m}
      # Embedding Model
      - EMBEDDING_MODEL=nomic-embed-text:latest
      - ENABLE_RERANKER=true
      - RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
      - RETRIEVAL_TOP_K=10
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=WARNING
      - MAX_UPLOAD_SIZE=80                # Max file upload size in MB
      # Phase 2: High-Impact Retrieval Improvements
      - ENABLE_HYBRID_SEARCH=true         # BM25 + Vector with RRF fusion
      - RRF_K=60                          # Reciprocal Rank Fusion k parameter (optimal: 60)
      - ENABLE_CONTEXTUAL_RETRIEVAL=false # Anthropic method: 49% reduction in retrieval failures (WARNING: 85% slower preprocessing when enabled)
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      redis:
        condition: service_started
      chromadb:
        condition: service_started
      rag-server:
        condition: service_healthy
    networks:
      - private
      - public
    volumes:
      - ./data/indexed_documents:/app/documents
      - docs_repo:/tmp/shared
      - huggingface_cache:/home/appuser/.cache/huggingface
      - documents_data:/data/documents
    secrets:
      - ollama_config

# Network configuration for security isolation
networks:
  public:
    driver: bridge
  private:
    driver: bridge
    internal: true

# Persistent volumes
volumes:
  chromadb_data:
    driver: local
  docs_repo:
    driver: local
  huggingface_cache:
    driver: local
  documents_data:
    driver: local

# Secrets management
secrets:
  ollama_config:
    file: ./secrets/ollama_config.env 