{
  "version": "v2",
  "summary": "Evaluation v2 replaces legacy evaluation module with a new DeepEval-based pipeline and expanded retrieval/citation metrics.",
  "requests": [
    {
      "endpoint": "POST /query",
      "old_fields": [
        "query",
        "session_id",
        "is_temporary"
      ],
      "new_fields": [
        "include_chunks"
      ],
      "notes": "include_chunks is optional (default false). When true, sources include chunk metadata for evaluation."
    }
  ],
  "settings": [
    {
      "name": "eval.citation_format",
      "values": [
        "numeric"
      ],
      "notes": "Controls how explicit citations are extracted from answers when include_chunks=true."
    },
    {
      "name": "eval.citation_scope",
      "values": [
        "retrieved",
        "explicit"
      ],
      "notes": "Controls whether citation metrics use retrieved chunks or explicit citations."
    },
    {
      "name": "eval.abstention_phrases",
      "values": [
        "list of strings"
      ],
      "notes": "Phrases used to detect abstention for unanswerable accuracy scoring."
    }
  ],
  "responses": [
    {
      "endpoint": "POST /query",
      "field": "sources[]",
      "old_fields": [
        "document_id",
        "document_name",
        "excerpt",
        "full_text",
        "path",
        "score"
      ],
      "new_fields": [
        "chunk_id",
        "chunk_index"
      ],
      "notes": "chunk fields are present when include_chunks=true."
    },
    {
      "endpoint": "POST /query",
      "field": "citations",
      "old_fields": [],
      "new_fields": [
        "citations"
      ],
      "notes": "Optional list of explicit citations, if provided by the server."
    }
  ],
  "metrics": [
    {
      "old": "contextual_precision",
      "new": "precision_at_k",
      "notes": "Retrieval precision is now computed using gold evidence or gold document IDs."
    },
    {
      "old": "contextual_recall",
      "new": "recall_at_k",
      "notes": "Retrieval recall uses gold evidence coverage (or gold documents when evidence is missing)."
    },
    {
      "old": "none",
      "new": "citation_precision",
      "notes": "New metric: fraction of cited chunks that match gold evidence/doc IDs."
    },
    {
      "old": "none",
      "new": "citation_recall",
      "notes": "New metric: fraction of gold evidence covered by cited chunks."
    },
    {
      "old": "none",
      "new": "mrr",
      "notes": "New metric: mean reciprocal rank."
    },
    {
      "old": "none",
      "new": "ndcg",
      "notes": "New metric: normalized discounted cumulative gain."
    }
  ],
  "cli": [
    {
      "old": "python -m evaluation.cli eval",
      "new": "python -m evaluation_v2.cli run <dataset>",
      "notes": "Legacy eval CLI removed."
    },
    {
      "old": "python -m evaluation.benchmark_cli run <dataset>",
      "new": "python -m evaluation_v2.cli run <dataset>",
      "notes": "Benchmark CLI consolidated into evaluation_v2."
    },
    {
      "old": "none",
      "new": "python -m evaluation_v2.cli run <dataset> --export-review <path> [--export-review-format csv|json]",
      "notes": "Export manual review data with answers, citations, and gold evidence."
    }
  ],
  "datasets": [
    {
      "old": [
        "squad",
        "ragbench",
        "hotpotqa"
      ],
      "new": [
        "ragbench",
        "qasper",
        "squad_v2",
        "hotpotqa",
        "ms_marco"
      ]
    }
  ]
}
