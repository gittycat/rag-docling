# Local RAG System - Product Requirements Document (PRD)

This document is edited manually.
It is a high level functional definition of the project.

## 1. Introduction

### 1.1 Overview
This document outlines a local Retrieval Augmented Generation (RAG) system for searching through a user's documents using natural language queries. Built with FastAPI, ChromaDB, and Ollama LLM.


Key objectives include:
1.  Single-user, local deployment.
2.  A public-facing web application for user interaction, with an internal-only backend for security.
3.  Simple setup and configuration, managed via Docker Compose.
4.  A clean, modern, and intuitive User Interface (UI).

The system will leverage a FastAPI front-end, a Python-based RAG server, ChromaDB for vector storage, and integrate with external services like Ollama for LLM capabilities and local file system for document sources, all containerized and orchestrated by Docker Compose.

### 1.2 Purpose
The purpose of this document is to define the product requirements for the local RAG system. It specifies the system's functionality, architecture, user interface, security considerations, and other non-functional aspects to guide its development. The primary goal is to enable a single user to easily deploy and utilize a powerful RAG pipeline on their local machine for querying personal or private data sources.

**Scope:** The project encompasses the development of:
- A FastAPI web application serving as the user interface.
- A RAG server handling the core logic, including interactions with Ollama and local file system.
- Integration with ChromaDB for local embedding storage and retrieval.
- A secure internal network for backend services.
- Management of secrets (e.g., Ollama settings) using Docker Secrets.

## 2. Problem Statement

Many individuals accumulate vast amounts of personal or private documents (notes, articles, research papers, etc.) across various formats and locations. Accessing and extracting specific information from these collections can be cumbersome and inefficient using traditional search methods. Users need an intelligent and intuitive way to query their documents using natural language, leveraging the power of Large Language Models (LLMs) for enhanced understanding and contextually relevant answers, without relying on cloud-based services and ensuring data privacy.

## 3. User Requirements

### 3.1 Target Users
- Individual users managing personal knowledge bases (e.g., researchers, developers, students, writers).
- Users requiring a private, local solution for document querying.
- Users comfortable with Docker for deployment.

### 3.2 User Stories
1.  As a user, I want to easily set up and deploy the RAG system on my local machine using Docker Compose so that I can start querying my documents quickly.
2.  As a user, I want to interact with the system through a web browser using a clean and intuitive interface so that I can ask questions and view results without needing command-line skills.
3.  As a user, I want to ask questions in natural language about my documents and receive relevant, context-aware answers generated by an LLM.
4.  As a user, I want the system to connect to my local Ollama instance for LLM processing so that my data remains private.
5.  As a user, I want the system to access my local files (markdown, txt, pdf, docx) as knowledge sources.
6.  As a user, I want to manage the documents indexed by the system, including adding new documents and deleting existing ones, through an admin interface.
7.  As a user, I want my Ollama settings to be handled securely using Docker Secrets, not hardcoded or exposed.
8.  As a user, I want the search results to clearly indicate the source document and the relevant excerpt.

## 4. Functional Requirements

### 4.1 Core Functionality
-   **Document Ingestion & Processing**: The system must be able to ingest supported document types (markdown, txt, pdf, docx), process them into text, and generate embeddings.
-   **Embedding Storage & Retrieval**: Employs ChromaDB to store and efficiently retrieve document embeddings.
-   **Query Handling**: Accepts user queries via the FastAPI web UI.
-   **RAG Pipeline Orchestration**:
    -   Converts user queries into embeddings.
    -   Retrieves relevant document chunks from ChromaDB based on query embedding similarity.
    -   Constructs prompts for an LLM using the retrieved context and the user's query.
    -   Sends prompts to a local LLM (via Ollama) for answer generation.
-   **External Service Integration**:
    -   **Ollama**: Interfaces with a local Ollama instance for LLM inference.
    -   **Local File System**: Reads and processes documents from a user-specified directory.
-   **Document Management**: Provides an admin interface for users to view and delete indexed documents.
-   **Result Presentation**: Displays search results including document name, matching excerpt, and a link/reference to the full document.

### 4.2 Service Breakdown

| Service        | Role                                                           | Network           | Secrets                         |
|----------------|----------------------------------------------------------------|-------------------|---------------------------------|
| FastAPI Web    | Serves UI pages, handles user input, communicates with RAG server | public + private  | Ollama settings (if any, e.g. endpoint) |
| RAG Server     | Business logic, orchestrates embedding lookups, LLM requests, document processing | private           | Ollama settings (if any, e.g. endpoint) |
| ChromaDB       | Local vector database for storing and retrieving embeddings       | private           | None                            |

### 4.3 User Interface

#### 4.3.1 Global Layout
-   A consistent top menu bar shall be present on the Home, Admin, and About pages.
-   The menu shall display: `Home | Admin | About`.
-   The label corresponding to the currently active page in the top menu shall be styled to indicate it is active/disabled as a link.

#### 4.3.2 Page Details

-   **Home Page**:
    -   Contains a prominent text input box with a placeholder (e.g., "Enter your questionâ€¦").
    -   Includes a "Search" button. Clicking this button submits the query and navigates the user to the Results Page.
    -   Top menu (Home | Admin | About) visible, with "Home" disabled.

-   **Results Page**:
    -   Displays a list of search results. Each result item should include:
        -   Document name/identifier.
        -   The relevant excerpt from the document that matches the query.
        -   A link or reference to access the full document (if applicable).
    -   Includes a "Return" or "Back to Home" button to navigate back to the Home Page.
    -   No top menu visible on this page to maximize focus on results.

-   **Admin Page**:
    -   Displays a table of currently indexed documents.
    -   Each row in the table should represent an indexed document and include:
        -   Document identifier (e.g., name or path).
        -   "Delete" action/button.
    -   Clicking a "Delete" action should trigger a confirmation dialog before proceeding with the deletion.
    -   Top menu (Home | Admin | About) visible, with "Admin" disabled.

-   **About Page** (Implied by Global Layout, content TBD):
    - Provides information about the application, its version, and potentially links to source code or documentation.
    - Top menu (Home | Admin | About) visible, with "About" disabled.

## 5. Non-Functional Requirements

### 5.1 Performance
-   **Query Response Time**: For local queries (after initial indexing), the system should aim for sub-second response times for displaying initial results, excluding LLM generation time which depends on the model and hardware. LLM generation should still be reasonably fast.
-   **Indexing Speed**: Efficiently process and index new documents. (Specific benchmarks TBD).

### 5.2 Reliability
-   **Data Persistence**: User-indexed data (embeddings, document metadata) must be persisted in Docker volumes to ensure recoverability on application restart or Docker Compose down/up cycles.
-   **Service Availability**: Services should be robust and restart gracefully if they encounter issues.

### 5.3 Maintainability
-   **Configuration**: The system should be configurable primarily via environment variables and the `docker-compose.yml` file (e.g., Ollama endpoint, ports).
-   **Modularity**: Codebase should be well-structured and modular to facilitate updates and bug fixes.

### 5.4 Usability
-   **Intuitive Interface**: The web UI should be intuitive, requiring minimal clicks to perform common actions (querying, viewing results).
-   **Responsive Design**: The web UI should be responsive and usable on typical desktop screen sizes.
-   **Clear Feedback**: The system should provide clear feedback to the user during operations (e.g., "Indexing in progress...", "Querying...").

### 5.5 Security
-   **Network Isolation**:
    -   Only the FastAPI web application should be exposed on a public network (host's network accessible via browser).
    -   All other backend services (RAG Server, ChromaDB) must reside on a private, internal Docker network, inaccessible directly from outside the Docker environment.
-   **Secrets Management**:
    -   Sensitive information (e.g., any future API keys for Ollama if it's a remote instance or requires auth) must be managed using Docker Secrets.
    -   Secrets should be mounted into containers at runtime and not baked into Docker images.
-   **Least Privilege**:
    -   No hard-coded credentials within the application code or Docker images.
    -   Services should be configured to access only the resources they explicitly require.

## 6. Technical Requirements

### 6.0 Coding Strategy
- **Split work into tasks**: 

### 6.1 Implementation Details
-   **Orchestration**: Docker Compose will be used to define and manage the multi-container application (local deployment only).
-   **Web Application Framework**: FastAPI (Python) for the front-end web application and API endpoints.
-   **RAG Server**: Python for the backend RAG logic.
-   **Vector Database**: ChromaDB for local embedding storage and similarity search with native Ollama integration.
-   **Embedding Generation**: ChromaDB's built-in `OllamaEmbeddingFunction` using the `nomic-embed-text` model for specialized text embeddings (768 dimensions).
-   **LLM Integration**: Ollama for running local LLMs. The specific models to be supported by default are TBD (e.g., Llama 3, Phi-3).
-   **Knowledge Source Integration**: Local file system (details of integration, e.g., reading markdown, pdf, docx, txt files from a user-specified directory, TBD).
-   **Networking**: Custom Docker networks to ensure public access for the UI and private internal communication for backend services.
-   **Styling**: Tailwind CSS for modern, responsive UI components.
-   **Development Approach**: Test Driven Development (TDD) - tests must be written before implementing any new functions or features. Write code incrementally, in small chunks of Edit-Test loops.
-   **Versions**: Use latest stable versions of Python libraries and Docker images at the time of implementation.

### 6.2 Interaction Flow
1.  User enters a query in the FastAPI Web App UI and submits.
2.  FastAPI Web App sends the query to the RAG Server via the private Docker network.
3.  RAG Server:
    a.  Uses ChromaDB's native Ollama integration to generate embeddings for the user's query using the `nomic-embed-text` model.
    b.  Queries ChromaDB with the query text (ChromaDB handles embedding generation automatically).
    c.  Retrieves the content of relevant document chunks.
    d.  Constructs a prompt using the retrieved context and the original query.
    e.  Sends the prompt to the configured Ollama instance for response generation.
    f.  Receives the generated response from Ollama.
4.  RAG Server returns the response (and source information) to the FastAPI Web App.
5.  FastAPI Web App displays the formatted results to the user.

**Local File System Integration (Conceptual):**
-   The RAG server will have a mechanism (TBD) to access and process files from a user-specified local directory. This might involve periodic scanning and indexing of markdown, pdf, docx, and txt files using ChromaDB's Ollama embedding integration.

**Embedding Strategy:**
-   **ChromaDB Native Integration**: The system uses ChromaDB's built-in `OllamaEmbeddingFunction` with the `nomic-embed-text` model for both document indexing and query processing.
-   **Consistent Vector Space**: Both documents and queries use the same embedding model to ensure optimal similarity matching.
-   **Specialized Model**: `nomic-embed-text` is optimized specifically for text embeddings (768 dimensions) rather than general-purpose LLM models.

## 7. Dependencies
-   **Core**: Docker, Docker Compose
-   **Python Backend**: Latest stable Python version (3.11+ recommended), latest versions of 
    - FastAPI
    - Uvicorn
    - Pydantic
    - ChromaDB client
    - requests/httpx (for Ollama)
    - [Microsoft MarkItDown](https://github.com/microsoft/markitdown) for document convertion from .txt, .pdf, .docx to .md
    - Other libraries for document loading (e.g., `langchain`, `llamaindex`)
    - ollama Python package (for ChromaDB's OllamaEmbeddingFunction).
-   **Frontend**: HTML with Tailwind CSS, JavaScript (vanilla JS or simple framework).
-   **Services**: Ollama instance (must be running and accessible by the RAG server with `nomic-embed-text` model available), ChromaDB (run as a Docker service using latest stable Docker images).
-   **Development**: pytest (latest version) for TDD testing, Docker and Docker Compose (latest stable versions).

### 7.1 Embedding Requirements
-   **Ollama Models**: The `nomic-embed-text` model must be available in the local Ollama instance for embedding generation.
-   **ChromaDB Integration**: Uses ChromaDB's native `OllamaEmbeddingFunction` for automatic embedding generation.
-   **Python Dependencies**: The `ollama>=0.4.4` Python package is required for ChromaDB's Ollama integration.
-   **Network Configuration**: Ollama embeddings endpoint must be accessible at `http://host.docker.internal:11434/api/embeddings` from the Docker environment.

### 7.2 Model Setup
Users must install the required embedding model in their local Ollama instance before running the RAG system:
```bash
# Install the nomic-embed-text model for embeddings
ollama pull nomic-embed-text
```

**Model Benefits:**
-   **Specialized for Embeddings**: `nomic-embed-text` is optimized specifically for text embeddings rather than general-purpose language modeling.
-   **Efficient Dimensions**: Uses 768-dimensional embeddings vs 4096+ for general LLMs, reducing storage and compute requirements.
-   **Consistent Performance**: Provides consistent embedding quality for both document indexing and query processing.

## 8. Design Details

### 8.1 Proposed Directory Structure (Illustrative)
```
/rag-system-project
|-- docker-compose.yml
|-- README.md
|-- /services
|   |-- /fastapi_web_app
|   |   |-- Dockerfile
|   |   |-- requirements.txt
|   |   |-- main.py
|   |   |-- /static
|   |   |   |-- /css (Tailwind CSS files)
|   |   |   |-- /js
|   |   |-- /templates
|   |   |-- /tests
|   |-- /rag_server
|   |   |-- Dockerfile
|   |   |-- requirements.txt
|   |   |-- main.py
|   |   |-- /core_logic
|   |   |-- /utils
|   |   |-- /tests
|   |-- /chromadb
|   |   |-- Dockerfile (if custom config needed)
|   |   |-- /config
|-- /secrets_template
|   |-- obsidian_token.txt (example)
|   |-- ollama_config.env (example, if needed)
|-- /data
|   |-- /chroma_db_data (volume mount)
|   |-- /indexed_documents (volume mount for originals or cache)
|-- /docs
|   |-- prd.md (this document)
```

### 8.2 Key Services & Responsibilities
(Refer to table in Section 4.2 Service Breakdown)

## 9. Testing Approach

### 9.1 Test-Driven Development (TDD) Methodology
-   **TDD Requirement**: For every new code function or feature, tests MUST be written first before implementation.
-   **TDD Process**: 
    1. Write a failing test that describes the desired behavior
    2. Implement the minimal code to make the test pass
    3. Refactor the code while keeping tests passing
    4. **Important**: Do NOT modify tests to make failing code pass; fix the code instead.
-   **No CI/CD**: All testing will be run locally during development; no continuous integration pipelines will be implemented.

### 9.2 Test Organization
-   **Unit Tests**: Located in `tests` subdirectories within each service (`/services/fastapi_web_app/tests`, `/services/rag_server/tests`).
-   **Integration Tests**: A separate test suite/directory (e.g., `/tests/integration`) to test interactions between services.
-   **Test Structure**: Each service maintains its own isolated test suite with appropriate mocking of external dependencies.

### 9.3 Testing Techniques
-   **Unit Testing**: Focus on individual functions and classes within the FastAPI app (endpoint handlers, utility functions) and RAG server (RAG pipeline steps, document processing logic).
-   **Integration Testing**:
    -   Verify the end-to-end flow from UI query submission to result display.
    -   Test interactions between FastAPI Web App, RAG Server, and ChromaDB.
    -   Use Docker Compose to set up the test environment with all services locally.
-   **Mocking**: Mock external dependencies like Ollama and local file system during unit and some integration tests to ensure predictability and avoid reliance on external factors.
-   **API Testing**: Test FastAPI endpoints directly using a tool like `pytest-fastapi` or `httpx`.

### 9.3 Test Scenarios (Examples)
-   **Core RAG Flow**:
    -   Successful query returning relevant results and sources.
    -   Query for which no relevant documents exist.
    -   Querying with different LLMs configured in Ollama (if supported).
-   **Document Management**:
    -   Indexing new documents (various supported types).
    -   Deleting indexed documents via the Admin UI.
    -   Ensuring deletion is reflected in subsequent searches.
-   **UI Interactions**:
    -   Navigation between Home, Results, Admin, About pages.
    -   Form submission (search query).
    -   Admin page: document list rendering, delete confirmation dialog.
-   **Error Handling**:
    -   Graceful handling of Ollama unavailability.
    -   Graceful handling of ChromaDB errors.
    -   Graceful handling of local file system errors (e.g., file not found, permission issues).
    -   Invalid user input (e.g., empty query).
-   **Security**:
    -   Verification that secrets are loaded correctly from Docker Secrets and not exposed.
    -   (Manual/configuration review) Verification of network isolation.
    -   Verification that local file system access is restricted to the user-specified directory.

### 9.4 Running Tests
-   **Python Unit Tests**: Using `pytest` command in the respective service directories (`/services/fastapi_web_app/tests`, `/services/rag_server/tests`).
-   **Integration Tests**: Using `pytest` with a setup that orchestrates Docker Compose to bring up the necessary services in a controlled local environment.
-   **Local Testing Only**: All tests run locally with no cloud dependencies or CI/CD integration.

## 10. Deliverables
-   `docker-compose.yml` file for local deployment of the system.
-   Each service in its own directory under `/services/`:
    -   FastAPI web application with Dockerfile, requirements.txt, and source code
    -   RAG server with Dockerfile, requirements.txt, and source code  
    -   ChromaDB configuration (if needed)
-   Static assets including Tailwind CSS for modern, responsive UI
-   Comprehensive test suites following TDD methodology for each service
-   A `secrets_template` directory showing the structure for required secrets
-   A comprehensive `README.md` file with detailed local setup, configuration, and usage instructions
-   This Product Requirements Document (`PRD2.md`)
-   **Note**: No cloud deployment configurations or CI/CD pipelines will be provided

## 11. Phases (Example)
1.  **Phase 1**:
    -   Project setup: Git repository, base Docker Compose file for local deployment.
    -   Define basic Docker network configuration (public/private).
    -   Set up service directory structure under `/services/`.
    -   Create Dockerfiles and requirements.txt for each service using latest stable versions.
    -   Initial ChromaDB integration (service in compose, basic connection).
    -   Set up Tailwind CSS integration for modern UI styling.
2.  **Phase 2**:
    -   **TDD Approach**: Write tests first for UI components and basic endpoints.
    -   Build Home Page UI (HTML with Tailwind CSS, JS for query input and submission).
    -   Build Results Page UI (HTML with Tailwind CSS, JS for displaying results).
    -   Implement basic query flow from FastAPI to RAG server (placeholder response initially).
    -   Ensure all new code passes the tests written first.
3.  **Phase 3**:
    -   **TDD Approach**: Write tests for RAG pipeline components before implementation.
    -   Implement core RAG logic in RAG server:
        -   ChromaDB's native Ollama integration for automatic embedding generation using `nomic-embed-text` model.
        -   ChromaDB querying with query texts (automatic embedding generation).
        -   Context retrieval from relevant document chunks.
        -   Ollama integration for LLM response generation.
    -   Connect RAG server response back to FastAPI and Results Page.
    -   Basic document indexing mechanism using ChromaDB's Ollama embedding function (e.g., from a local folder).
    -   Fix implementations until all tests pass (do not modify tests).
4.  **Phase 4**:
    -   **TDD Approach**: Write tests for admin functionality before implementation.
    -   Implement Admin Page UI (document listing, delete functionality with confirmation) using Tailwind CSS.
    -   Implement backend logic for document deletion in RAG server and ChromaDB.
    -   Integrate Docker Secrets for managing configurations.
    -   Ensure responsive design with Tailwind CSS.
    -   Fix implementations until all tests pass.
5.  **Phase 5**:
    -   Complete test coverage: ensure all functions have tests written first per TDD.
    -   Run comprehensive local testing: unit tests, integration tests for all key features.
    -   Write comprehensive `README.md` documentation for local setup only.
    -   Final UI/UX polish using Tailwind CSS and bug fixing.
    -   Code review and refactoring while maintaining test coverage.

## 12. Future Enhancements

### 12.1 Potential Features
-   **Multi-user support**: Authentication and authorization for multiple users.
-   **Expanded Document Support**: Support for a wider range of document types (e.g., DOCX, PPTX) and sources (e.g., web URLs).
-   **Advanced RAG Techniques**: Implementation of more sophisticated RAG strategies (e.g., re-ranking, query transformation).
-   **LLM Model Management**: UI for selecting or configuring different LLMs available through Ollama.
-   **Automated Document Ingestion**: Watch folders for new documents to automatically index.
-   **Knowledge Graph Integration**: Building or connecting to a knowledge graph for more structured data retrieval.
-   **Improved Admin Panel**: More advanced features like re-indexing specific documents, viewing indexing status, managing users (if multi-user).
-   **Feedback Mechanism**: Allow users to provide feedback on the quality of search results.
-   **Batch Querying**: Allow users to upload a file of queries and get results.
