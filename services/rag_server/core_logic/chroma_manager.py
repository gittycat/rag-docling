import os
from typing import List, Dict
import chromadb
from langchain_chroma import Chroma
from core_logic.embeddings import get_embedding_function

COLLECTION_NAME = "documents"

def get_chroma_client():
    """Get ChromaDB HTTP client for direct access when needed"""
    chroma_url = os.getenv("CHROMADB_URL", "http://chromadb:8000")
    host = chroma_url.replace("http://", "").replace("https://", "").split(":")[0]
    port = int(chroma_url.split(":")[-1])
    return chromadb.HttpClient(host=host, port=port)


def get_or_create_collection():
    """
    Get or create the documents collection using LangChain Chroma vectorstore.
    Returns a Chroma vectorstore instance.

    This now uses LangChain's Chroma wrapper for better integration with
    LangChain retrieval patterns.
    """
    client = get_chroma_client()
    embedding_function = get_embedding_function()

    vectorstore = Chroma(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding_function=embedding_function
    )

    return vectorstore


def add_documents(collection, documents: List[str], metadatas: List[Dict], ids: List[str]):
    """
    Add documents to ChromaDB collection using LangChain Chroma interface.
    Embeddings are automatically generated by the vectorstore's embedding function.

    Args:
        collection: Chroma vectorstore instance
        documents: List of document texts
        metadatas: List of metadata dicts
        ids: List of unique document IDs
    """
    # LangChain Chroma uses add_texts method
    collection.add_texts(
        texts=documents,
        metadatas=metadatas,
        ids=ids
    )


def query_documents(collection, query_text: str, n_results: int = 5) -> Dict:
    """
    Query the collection for similar documents using LangChain Chroma.
    Query embedding is automatically generated by the vectorstore's embedding function.

    Args:
        collection: Chroma vectorstore instance
        query_text: Search query text
        n_results: Number of results to return

    Returns:
        Dictionary with documents, metadatas, distances, and ids
        (compatible with previous ChromaDB format)
    """
    # Use similarity_search_with_score for compatible results
    results_with_scores = collection.similarity_search_with_score(
        query=query_text,
        k=n_results
    )

    # Convert LangChain format to ChromaDB-compatible format
    documents = []
    metadatas = []
    distances = []
    ids = []

    for doc, score in results_with_scores:
        documents.append(doc.page_content)
        metadatas.append(doc.metadata)
        # ChromaDB uses distance (lower is better), score from similarity_search is similarity (higher is better)
        # Convert similarity to distance: distance = 1 - similarity (approximate)
        distances.append(1.0 - score if score <= 1.0 else score)
        ids.append(doc.metadata.get('id', ''))

    # Return in ChromaDB query format (nested lists for batch queries)
    return {
        'documents': [documents],
        'metadatas': [metadatas],
        'distances': [distances],
        'ids': [ids]
    }


def delete_document(collection, document_id: str):
    """
    Delete a document from the collection by ID.

    Args:
        collection: Chroma vectorstore instance
        document_id: Unique document ID to delete (will delete all chunks with this document_id)
    """
    # Get underlying ChromaDB collection for deletion
    chroma_collection = collection._collection

    # Query for all chunks with this document_id
    results = chroma_collection.get(
        where={"document_id": document_id}
    )

    # Delete all matching chunk IDs
    if results and results['ids']:
        chroma_collection.delete(ids=results['ids'])


def list_documents(collection) -> List[Dict]:
    """
    List all documents in the collection.
    Groups chunks by document_id and counts them.

    Args:
        collection: Chroma vectorstore instance

    Returns:
        List of dictionaries with id, metadata, and chunk count for each document
    """
    # Access underlying ChromaDB collection
    chroma_collection = collection._collection

    # Get all documents
    results = chroma_collection.get()

    # Group chunks by document_id
    doc_map = {}
    for i, chunk_id in enumerate(results['ids']):
        metadata = results['metadatas'][i] if i < len(results['metadatas']) else {}
        doc_id = metadata.get('document_id', chunk_id)

        if doc_id not in doc_map:
            doc_map[doc_id] = {
                'id': doc_id,
                'file_name': metadata.get('file_name', 'Unknown'),
                'file_type': metadata.get('file_type', ''),
                'path': metadata.get('path', ''),
                'chunks': 0
            }

        doc_map[doc_id]['chunks'] += 1

    return list(doc_map.values())
