"""DeepEval test case factory for RAG evaluation.

This module provides utilities for creating LLMTestCase objects
from RAG query results for evaluation with DeepEval metrics.
"""

from typing import List, Optional
from deepeval.test_case import LLMTestCase


def create_test_case(
    question: str,
    actual_output: str,
    expected_output: str,
    retrieval_context: List[str],
    context: Optional[List[str]] = None,
    name: Optional[str] = None,
    tags: Optional[List[str]] = None,
) -> LLMTestCase:
    """Create a DeepEval test case from RAG query results.

    Args:
        question: User's input query
        actual_output: Response generated by the RAG system
        expected_output: Expected/ground truth answer from golden dataset
        retrieval_context: List of retrieved document chunks (for retrieval metrics)
        context: Optional additional context (defaults to retrieval_context if not provided)
        name: Optional unique identifier for the test case
        tags: Optional tags for categorization (e.g., ["factual", "easy"])

    Returns:
        LLMTestCase configured for RAG evaluation

    Example:
        >>> test_case = create_test_case(
        ...     question="What is Python?",
        ...     actual_output="Python is a programming language.",
        ...     expected_output="Python is a high-level programming language.",
        ...     retrieval_context=["Python is a programming language..."],
        ... )
    """
    # Use retrieval_context as context if not provided
    if context is None:
        context = retrieval_context

    return LLMTestCase(
        input=question,
        actual_output=actual_output,
        expected_output=expected_output,
        retrieval_context=retrieval_context,
        context=context,
        name=name,
        tags=tags,
    )


def create_test_cases_from_dataset(
    dataset: List[dict],
    rag_results: List[dict],
) -> List[LLMTestCase]:
    """Create multiple test cases from golden dataset and RAG results.

    Args:
        dataset: List of golden Q&A pairs with structure:
            [{"question": "...", "expected_answer": "...", "tags": [...], ...}]
        rag_results: List of RAG query results with structure:
            [{"response": "...", "source_nodes": [...], ...}]

    Returns:
        List of LLMTestCase objects

    Example:
        >>> golden_dataset = [
        ...     {"question": "What is Python?", "expected_answer": "A language."},
        ...     {"question": "What is AI?", "expected_answer": "Artificial Intelligence."},
        ... ]
        >>> rag_results = [
        ...     {"response": "Python is a language.", "source_nodes": ["..."]},
        ...     {"response": "AI is artificial intelligence.", "source_nodes": ["..."]},
        ... ]
        >>> test_cases = create_test_cases_from_dataset(golden_dataset, rag_results)
    """
    if len(dataset) != len(rag_results):
        raise ValueError(
            f"Dataset and RAG results length mismatch: {len(dataset)} vs {len(rag_results)}"
        )

    test_cases = []
    for golden, rag_result in zip(dataset, rag_results):
        test_case = create_test_case(
            question=golden["question"],
            actual_output=rag_result.get("response", ""),
            expected_output=golden.get("expected_answer", ""),
            retrieval_context=rag_result.get("source_nodes", []),
            name=golden.get("id"),
            tags=golden.get("tags"),
        )
        test_cases.append(test_case)

    return test_cases


def create_test_case_from_response(response_data: dict) -> LLMTestCase:
    """Create test case from RAG API response format.

    Args:
        response_data: RAG API response with structure:
            {
                "query": "...",
                "answer": "...",
                "sources": [{"excerpt": "...", ...}, ...],
                "expected_answer": "..."  # From golden dataset
            }

    Returns:
        LLMTestCase for evaluation

    Example:
        >>> response = {
        ...     "query": "What is Python?",
        ...     "answer": "Python is a programming language.",
        ...     "sources": [{"excerpt": "Python is..."}, {"excerpt": "..."}],
        ...     "expected_answer": "Python is a high-level language."
        ... }
        >>> test_case = create_test_case_from_response(response)
    """
    # Extract source excerpts for retrieval context
    retrieval_context = [
        source.get("excerpt", source.get("full_text", ""))
        for source in response_data.get("sources", [])
    ]

    return create_test_case(
        question=response_data["query"],
        actual_output=response_data["answer"],
        expected_output=response_data.get("expected_answer", ""),
        retrieval_context=retrieval_context,
    )
